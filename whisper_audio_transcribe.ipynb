{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "authorship_tag": "ABX9TyNeff5Ijna5y0uvKLnjw47U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raviakasapu/LLM-Training-Docs/blob/main/whisper_audio_transcribe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZDoQ7J25qap",
        "outputId": "a157531e-a6a9-4ba9-caac-971df151e867"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-qcehbq1f\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-qcehbq1f\n",
            "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.2)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper==20231117)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117) (3.13.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2023.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=802825 sha256=e723d86c1e11c26955e0b2ebae240dfae84dbe1447d400d8bb3b8bc97765d078\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-c50iwh3h/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: tiktoken, openai-whisper\n",
            "Successfully installed openai-whisper-20231117 tiktoken-0.6.0\n",
            "Collecting pytube\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytube\n",
            "Successfully installed pytube-15.0.0\n"
          ]
        }
      ],
      "source": [
        "# prompt: install whisper and pytube\n",
        "\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install pytube\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytube as pt\n",
        "import whisper"
      ],
      "metadata": {
        "id": "9guUM1g06Aeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download mp3 from youtube video (Two Minute Papers)\n",
        "#yt = pt.YouTube(\"https://www.youtube.com/watch?v=SL0v3hk-rqk\") # SAP BTP Data Export API\n",
        "yt = pt.YouTube(\"https://www.youtube.com/watch?v=OZEYkQTp7x8&t\") # How He Built The Best 7B Params LLM with Maxime Labonne #43\n",
        "\n",
        "stream = yt.streams.filter(only_audio=True)[0]\n",
        "stream.download(filename=\"audio_english.mp3\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "NCXr5ocY6Co0",
        "outputId": "0d698619-b4f0-4775-981a-2503a02e5617"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/audio_english.mp3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = whisper.load_model(\"large\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYxrpdJ66FAi",
        "outputId": "a4bbab55-4bc2-46a6-f378-5939fcb829cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 2.88G/2.88G [01:02<00:00, 49.6MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.transcribe(\"audio_english.mp3\")\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOHOKD0b6Hai",
        "outputId": "e6b45289-f58c-4252-da09-87d762e853de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " And some people in 2023, they started doing some crazy things with the weights by averaging them and merging them. And most people saw that and were like, this is crazy, like this cannot work right until it really worked. And now the Merge models are completely dominating the OpenLLM leaderboard. The first step in this LLM pipeline is pre-training. And the second step is supervised fine-tuning. The third step, which is reinforcement learning from human feedback. Fourth step in this pipeline, merging models. So yeah, I would say you have these four steps, but realistically, you can only do supervised fine-tuning, preference enlightenment, or merging models. And I spent like years and years doing this indie game development. And I can say with certainty that it's a lot harder than AI. Instead of building one game, you can do GPT-3 from scratch by yourself. It's really extremely time-consuming. Doing AI is like pleasure compared to indie game development. So my advice would be build your stuff, ship your stuff. And this is probably the best way for me to learn about these topics. This episode is brought to you by Training Data. If you're new in data science and want to get into the field, or if you're new to the field, please subscribe to my channel. And if you're new to the field, please subscribe to my channel. And if you're new to the field, please subscribe to my channel. If you're already in the field, but want to progress, well, Training Data is the platform for you. They offer courses on feature engineering and selection, model tuning, interpretability, and much more. You will get both the math and the intuition behind each method, but also Python code ready to power your own projects. So if you're interested, visit the link in the description, and don't forget to use the code AISTORIES to get a 10% discount. Hello, everyone, and welcome. Welcome to the AI Stories podcast. I'm Neil Leiser. I'm a senior data scientist, and I'm going to be your host. So today, our guest is Maxim Labon. Maxim is an expert in GenAI. His course on LLM has now over 20K stars on GitHub, and he developed Neural Beagle 14 7B, the best performing 7 billion parameter model on the open LLM leaderboards. Prior to this, Maxim worked at Airbus. And he did a PhD in AI at Institut Polytechnique de Paris. So if you enjoyed this episode, please subscribe to the AI Stories YouTube channel, comment, share, and leave a five star review. Hi, Maxim. So, so excited to have you here with me today. First thing I want to ask is how did everything start? How did you get into the world of AI and machine learning? Hi, Neil. Thanks for having me here. I'm really happy to be here. Happy to be in the podcast. I think it's about like being curious about this field because I haven't studied it like in uni. I started doing my PhD actually. So I was very curious about machine learning and I had this opportunity of doing a PhD on machine learning applied to cybersecurity. Cybersecurity is the thing that I've studied. So it was quite exciting to see like how machine learning would spice things up. And that's how I started really. So yeah, tell me a bit more about AI applied to cybersecurity. Like how does this work? What was your first project about and why do you actually need AI for cybersecurity? There are a lot of things that you can do with AI and cybersecurity. My thesis was around intrusion detection systems using machine learning. So intrusion detection systems, they are like these antiviruses, but for computer networks and what they do is that they monitor network traffic. And they try to match what they observe with a database of signatures to detect cyber attacks. And that's how traditional intrusion detection systems work. But in this thesis, the idea was, hey, can we use fancy neural networks to make them even better? And so, OK, you do your PhD in AI and through cybersecurity. Why do you decide to stay in AI rather than going? I mean, you mentioned you studied. You studied cybersecurity. So why do you stay in AI rather than in cybersecurity? What do you like so much about AI? Yeah, that's a good question. I think that at the beginning of my PhD, I was like, well, actually, I really like AI and I really do not like cybersecurity. So to me, it was an epiphany. And one of the things that I like the most about the AI community was all the articles, all the educational resources that you can find online. And that. To me, it was really absent to learn more about cybersecurity. Not that there's nothing out there. It's just like not the same like diversity and quality and even quantity, really. So this is something that I really, really enjoyed about AI. And I became passionate about it. And I started reading all these articles, all these books, all this stuff. And yeah, that's why I'm still in AI, I guess, because I really fell in love with it during my PhD. So did you think you could? Like progress faster by staying in AI rather than going through to the cybersecurity path? Yeah, it was not even a question of progress. It was more about, yeah, I can really learn a lot about AI whenever I want. And that's really precious. And I found it a lot more interesting than cybersecurity in general. I after that, I find it quite boring. Like the stuff that I was doing. That's why I decided to continue with AI and do more general things, not just apply to cybersecurity, but try to diversify also my applications of it. Okay. So after your PhD, you work at Airbus for a bit. So do you want to talk a bit about Airbus? What was your job there and how did you use AI there? Maybe dive into one project that you worked on there? Yeah. So Airbus started to do an AI lab. So I was working with my university and with my PhD supervisor. So my PhD supervisor, Jamal Zeghlas, he told me, hey, this new lab they're hiring, maybe you should go there and talk to them. So that's what I did. And I was convinced that they were doing a lot of cool stuff. And to me, it was very reassuring because it was not AI applied to cybersecurity anymore, but it was AI applied to computer networks. And I had a lot of background with computer networks. So I felt. Quite secure that I could, you know, understand and do things that would be interesting and meaningful in this area. So that's why I decided to accept the offer and started working on a few projects related to machine learning and computer networks. And then we also diversified inside of the lab until it was not just about computer networks, but also a lot of different applications. Okay. So can you dive into one particular project that you think was interesting? My favorite project I did there was about applying large language models to network protocols. That was a pretty funny idea that I had during my PhD, because when you look at network protocols like HTTP, IP, SSH, you see that they have a syntax and like protocols, they're synonyms of languages. So it's not natural languages. It's artificial languages. Okay. But you can still apply a lot of NLP techniques to do these things. This is something that I've started doing in my PhD. And this is something that I wanted to do this time with large language models. So back then it was BERT, Roberta, GPT-2. The three was not out yet. And it was really cool to see how you can actually teach them network protocols and when they have a good understanding of these protocols, how to be able to. So we tried to pre-train this model. Terrible idea. Now everybody knows that it's a terrible idea to pre-train these models, but back then it was not so obvious. Transfer learning works really well. So actually, like even if you want to train your GPT-2 model over BERT on network protocols, it's better to just fine tune it and like really use an existing pre-trained model. And on top of that, we did a lot of things. Some of them related to cybersecurity, like anomaly detection. Some of them just network protocol understanding. So it's a classification task. And it's actually quite difficult. You might not. Yeah, it might not be obvious, but actually recognizing the protocol is a task that is not obvious. And a lot of people maintain heuristics to do it automatically. And it was really cool to see that these models, these large language models. They can outperform humans at this task just by seeing a lot and a lot of data. So overall, a really cool project. We installed our GPU server by ourselves. Everything, even like front end deployments. So it was really end to end. And so for people who aren't that familiar with network protocol, can you explain maybe more high level how this works? Like why do we need this classification? And what would be a practical example? Where your model would actually be used? Yeah, so you can look at this protocols like a way to transmit information between two machines. So the first thing that you're going to see in these protocols is the source address, destination address, maybe source port, destination port. A lot of metadata, like what's the priority of this message compared to other messages, for example. That's a good one. And what we could do. With these models. The case of GPT-2, the idea was, can we generate artificial synthetic data to augment our data set? That was one application. Otherwise, with protocol classification, the idea was, I would like to be able to like, I have a packet, but I don't know the protocol. I would like to be able to automatically classify it because if it's something like SSH, I want to process it one way. If it's something like HTTP. I want to do that. I want to do something else with it. So it's very high level view of it, but it allows you to build applications on top of this ability. Okay. And you realize already at that time that it could beat a human at this task? Yeah, it was really funny. Maybe because we're not so good at it. I mean, I did it for like a month and month and month. At some point I could also recognize protocols, but not as good. So it was really funny to see that in 2020. I was already like starting to be a big. Big. Big. Bigger believer in LLMs because this blew my mind. Okay. So already before pre-ChatGPT, you were already into LLMs. Yeah. I'm not one of the first ones, but I started in 2019 by fine tuning a GPT-2 model to write articles for me. That was my first idea as a PhD student. Unfortunately, it was not very good. I still had to write my own articles, but it was already quite funny because it could come up with ideas that were interesting. Like, you know, combinations of different ideas merged together and sometimes it made sense. Sometimes it didn't make a lot of sense. And so how, what did you think about ChatGPT when it came out? Because you were already playing with transformers, like were you surprised by ChatGPT or were you like, okay, I knew this was going to happen. We were, we were on that far. What were your thoughts on ChatGPT when it appeared? I was playing with OpenAI Playground. I've been playing with it. For a long time. I had like a bunch of stupid ideas like, hey, can I automate tweeting? It doesn't work at all, unfortunately. But yeah, so they released the Instruct version of GPT models earlier. And honestly, I, I liked them. It was, it was good. But to me, it was really not that great. Right. Like I would play with it in the playground, but you had to pay stuff like that. I. I didn't play with it that much. When ChatGPT was released, I was really surprised by how other people saw it because it became like a massive hit and the playground has been there for a long, long time. So I understand that it's in terms of product, like it's a completely different thing. But yeah, it was surprising to see that actually like people were really interested and you could build something really meaningful with it. And since then, like, yeah, I started using ChatGPT. Yeah. I have been using it for a long time. I think I can still use it pretty, almost every day. And yeah, this is, this is a pretty cool app. What do you frequently use ChatGPT for then? I mean, everyone uses it now or a lot of people, but I'm curious to understand, yeah. What do you use it for? I use it for two main purposes. The first one is to fix grammar. Well, I'm not sure about like a sentence. And I'm like, I don't know, I'm too French for it. So I'm going to just ask ChatGPT to, to, to make sure that it works. And then I'm going to use it for a lot of different things. And then I'm going to use it for a lot of different things. But yeah, it's a really cool app. But yeah, it's a really cool app. I mean, it's a really cool app. I'm really proud of ChatGPT, but yeah, I need to make sure that my, my sentence is correct. And otherwise there's codes. I think it's really good, coding assistant. And that's probably my main way of using it, to just describe what I want and to use the output. Maybe not directly, I probably need to fine tune it a little, but it's, it's doing the heavy lifting for me. And that is really relevant in, in most of my applications. Yeah. I, I like ChatGPT. Yeah. Yeah. to use cursor for code i don't know if you if you've heard of it or do you use it as well i've downloaded it and i've never tried it so i don't know but maybe i should give it a try yeah it's super cool because it's kind of a chat gpt integrated into your vs code so you've got github copilot that completes your code already but with cursor you can really chat with your codes like you can for example well the first thing is just asking to write code to replace code in your own repository but you can also kind of scan your entire repository and ask questions about it or for example chat gpt isn't up to date with langchain so if you've got questions about langchain it's not going to be able to answer but with cursor you can just give it the langchain documentation it's gonna scan it and learn from you and then you can just scan it and it's going to ask you questions about your code so i'm not sure if it's going to be able to answer but with cursor you can just give it the and then it can basically answer questions based on that as well. So very, very powerful. I've installed it and yeah, really like it. It's really boost my productivity. So recommend the tool. Thanks for the recommendation. Yeah, I'm going to give it a try. So yeah, I want to dive a bit more into Gen AI. I mean, I mentioned you've done a course on LLM, which has over 20K stars on GitHub. And yeah, I like the structure of your course. Maybe we can even follow this for the conversation. It has kind of three parts, the LLM fundamentals, which is how to get into Gen AI and learn about LLMs. There is the LLM scientist, which is how to train your own model, basically. And then there is the LLM engineer, which is more how to deploy those models in production. So, I mean, let me know if that was a good intro. Happy for you to have an intro on this as well. I want to start with the first part, LLM fundamentals. Like, how do we get into Gen AI? What resources would you recommend? What do we need to know? Yeah, Kintu, have your view on this. Yeah, so I started this LLM course because a lot of people started asking me last year that they, hey, how do I get into LLMs? And I had some resources I would share with them. And I decided, like, why not centralizing everything? At once, so this is how it started. In the first version, I had a lot of fundamentals and not so much about LLMs. So in the V2, like, this is something that I address with the three roadmaps that you've described. I would say that don't start with the fundamentals. You will need them when you need them, but start building something that interests you because the main driver is actually the motivation that you have to learn about this stuff. If you start with, let's say, a lot of fundamentals, if you start with math with calculus, you're probably not going to build any LLM application because you're going to be stuck with math for an entire year. So my advice would be try to find something that really interests you, that really motivates you. It can be fine-tuning a model. It can be building an app that you would like to use. But the idea of having a project, and when you get stuck because, like, you don't understand, you don't understand something, then you can go back to the fundamentals and learn more about this stuff. But otherwise, I would recommend just try to build your stuff, ship your stuff, and this is probably the best way, at least for me, to learn about these topics and not to see it really as a strict roadmap, but more as a, oh, I don't know about that, so I'm going to learn about it because I need it to do my project. Okay, so you would recommend start with a project, and if you're stuck or if there is something that you don't know about, then go and read about it. But otherwise, start with a project and don't spend ages trying to learn all the basic stuff like the math, the coding, because you're not going to build anything. Yeah, exactly. I don't think that, yeah, if you spend so much time with the fundamentals, okay, you're going to be good at math, but actually you don't need it that much when you build this stuff. You can do even a lot of research without having to do a lot of research. So really don't get stuck on that. And the best way is probably to start building your own project and when you need it, actually acquire these skills. So how did you start? Did you start with a project? Did you start by learning a bit of the fundamentals and then working on the project? Yeah, I started with my thesis, so it was a long project, but this is a bit like how I started. And then I mentioned the project I did at Airbus, and this is when I really started, learning a lot about this stuff. Hacking Face was a really great resource for that. They had a lot of tutorials, a lot of notebooks, and what I do today is really inspired by this experience, reading these notebooks, these articles, and because it was a very practical, hands-on way of doing stuff, and by doing stuff, learning about them. And this is why I try to re-transcribe in this course and also in my articles. Yeah. I started with, I think, one or two short courses, like a few videos of the... There is a deep learning... It's from deep learning... I don't even remember who now... Some PhD students at Berkeley, don't remember the exact name, but they've done a course on LLM, so I watched one or two lectures and then something on Coursera. And then after this, I just started my own project. And I just feel like when you start a project, you learn so much more. Like it's good to know a bit the fundamentals and some resources, good resources that you could look into, but you just learn much more by building something. And as you mentioned, even better if it's something you're really interested in, because you're going to be curious and you're just going to do more. Yeah, I think a good way of approaching it is really, okay, you want to find a new model, take this notebook that allows you to find a new model and start hacking it. And when you want to modify it, you will be forced to understand how it works. And this is really the beginning to help you to kickstart your project. And then you can delve deeper into it and really modify it until it's really your thing. Like you made it yours and by making it yours, you learned about how to do every single step. To me, this is how I tend to learn about these things. And so if you had to pick like a couple of reasons, I would say that the first one is that you have to have a lot of resources. I mean, your course, which is amazing, by the way, I really enjoy it. Like it's really complete. Like there is a bit of everything and also some deep topic that are not really talked about or not too much talked about. Like quantization is not something I've seen a lot, but I saw it in your course. But what are maybe a couple of resources that you would recommend? Yeah, I've recommended a lot of resources in this course, and I think that's probably like everything that Hugging Face makes because they have a lot of resources now over the years that can explain a lot of stuff about LLMs. And if you're more into research and you want to learn more about maybe the math and like having a real literature review of this stuff, there's a blog called Lil Log. It's a really good blog. And it's also like a great, great resource to learn a lot about LLMs in general. Okay, great. Thanks a lot. I'm going to share those in the description of the podcast. So let's move to the LLM scientists side of things. I mentioned you've built your own model, one of the best 7 billion parameter model on the OpenLLM leaderboard. So before we dive into this particular project and what you've done. Can you just highlight high level, what are the steps needed to build your own LLM? Let's say tomorrow I want to start building my own LLM. What should I do? Yeah, there are a lot of different steps that you can take and like different approaches. It really depends on what you mean by it's my LLM now. I think pretty much you can take it and call it yours because the first step in this LLM pipeline is pre-training. And that's not many people know about. Not many people can do because pre-training models that are that big, like 7 billion parameters, cost a lot of money. When it's 70 billion parameters, it costs like really, really a lot of money now. So there are only a few companies that can do it. And so we tend to take these pre-trained models. And the second step is supervised fine tuning. Supervised fine tuning, you're going to give the model pairs of instructions and outputs so it learns to follow instruction. It's also called instruction tuning because of that. And it's a really efficient way to leverage the weights that were learned during the pre-training phase and reuse them, improve them so the model becomes better and better. And for most of last year, that was really the only way that we dealt with these models because it was too costly to pre-train them. And the third step is to make sure that the models are in the right order. So we did a step which is reinforcement learning from human feedback, now also called preference alignment because we ditched the reinforcement learning part of it. That was the problematic part because it didn't really work. And this is why for most of last year, we didn't have a lot of models that were using reinforcement learning from human feedback until we got the direct preference optimization, also called DPO, which is the DPO algorithm. And that also kick-started a new era in the open source community where people started fine-tuning these models, not with supervised fine-tuning, but now with DPO. And what it does is that you provide examples of what you want and what you don't want as answers. And the model learns to only output the preferred answers and not the rejected ones. So it's a very simple way to make sure that the models are in the right order. And it's a very simple, efficient technique to align the model with what you want, with what you expect in terms of answers. This is also used often, like by OpenAI with ChatGPT, to censor the model. But this is not the only way of using it. And you can use it like supervised fine-tuning in order to just boost the performance of the model. And finally, I would say that now we have a fourth step in this pipeline. And this step is about merging models. Merging models is quite new. It's quite experimental. And it's based on the idea, hey, we have so many models on the Hugging Face Hub. What can we do with them? And some people in 2023, they started doing some crazy, crazy things with the weights by averaging them and merging them and creating these Franken merges by slapping layers of different models on top of each other. And I think that most of the models that we have now, we're going to use them. And I think that most people saw that and were like, this is crazy. Like, this cannot work right. Until it really worked. And now, thanks to Charles Goddard, we have the Merge Kit Library. And this allows you to easily merge a bunch of different models using different algorithms. And now, the merge models are completely dominating the OpenLLM leaderboard. So yeah, I would say you have these four steps. But realistically, you can only do supervised fine-tuning. Preference alignment, or merging models. Okay, so let's dive into each one of them. The first one, the pre-training, is the most difficult. And that's because, so my understanding is you basically train a model to predict the next word, usually. And you really need a lot, a lot of data and a lot of compute. So is that why it's so difficult for me, if I want to start tomorrow, to start to train a model? It's because it's going to be too expensive, take too long, and you need a lot of time. Yeah. Yeah. And you need a lot of resources. Absolutely. The answer is more complicated than that. But this is the first problem, right? Because you would need clusters, really, really big clusters of GPUs in order to do it. And then, where do you get the data? Right now, we have a lot of open source data sets that you can use. But they're really, really big. You need trillions of tokens in order to do that. And that's a massive volume of data, of course. So you need the GPUs. And even when you're not using GPUs, you're still going to need a lot of data. And you have all the hardware and the data. And you also have the algorithms, the model, the architecture of the model. These things keep crashing. I have this experience from Airbus, because I told you, I've pre-trained some models. And they were very small in comparison. But it doesn't work very well, right? The process is really error prone. And you can see it even in recent projects, like TinyLama. TinyLama was this project. And we have trained a 1.1 billion parameter LAMA model on a lot of tokens. And you can see their logs. Like, it keeps crashing. And you need to restart from the previous checkpoint. It's really not as straightforward as one might think. And this is also why we don't pre-train this model very often. Because even if you have all the resources, you still need to work a lot. It's not going to be just like, oh, I'm going to wait for one month. you'll still have a lot of work to do. And it's already expensive if you need to train it once. But if it fails and you need to restart again and again, it's really, really, really expensive. So that's why it's not... You don't restart from the beginning because you keep saving the... Like you have saviors of the models, but yeah, still you need to manually say, hey, I can come back to the previous checkpoint and manually check a lot of things. And then during training, you might realize, oh, there is a problem in my data because now my loss is spiking. So actually now you need to clean your data set and come back to training afterwards. Okay, so that's the first part. And it's usually very complex. There are a couple of companies and people in the world who can do it, but that's not accessible to everyone. So the second step that you mentioned is supervised fine tuning. Two questions here. First of all, why do we need supervised fine tuning? Why those models don't work alone? Why do we need supervised fine tuning? And yeah, how... Can we use it to make the model better? What are the techniques that we could use? Yeah, so as you said, during pre-training, we teach them to predict the next token. And after pre-training, the model that you have is called the base model. And this base model, it can do text completion. If you start a sentence, it will be able to complete it. But this is not ChartGPT. ChartGPT is one step further. And this is the idea also behind InstructGPT. It's the idea that actually this is good, but what we really want is some kind of assistant. You want to give it instructions. You want to ask questions and you want the LLM to answer your questions. In order to do that, you need supervised fine tuning. In supervised fine tuning, this is what you create in terms of dataset, dataset with questions or instructions and answers. So you're going to give the questions and you're going to ask questions. You're going to teach the model to output the answer that are expected. So this is the entire process and you keep iterating that. I have to say that you need a lot less data compared to the first step, which is why you can do it. It doesn't crash that often also, but maybe because it doesn't take as long. So that's the good part. And in terms of techniques, I would recommend checking Axolotl. It's a framework to do supervised fine tuning. Now it can also do the third step reference alignment, but it was really made by people in the open source community to fill this gap and have a reusable framework that would implement the latest techniques. For example, packing the data in order to minimize the footprint or to just have Qloa, Loa, all these parameter efficient techniques that allow you to train your models faster and using less time. So you can do that. So it's a lot more efficient than what you would use if you did a full fine tuning of the model. I hear a lot about Qloa, Loa. I haven't actually yet read an article about it. So why is it better than traditional fine tuning? How does this work? In terms of performance and accuracy, well, in terms of accuracy, it is not as good as full fine tuning because in full fine tuning, what you do is that you say, I'm going to train all the parameters in my model. Of course, it's really expensive, because you have a lot of parameters in your model, like 7 billion, as we said, or even more. So some people came up with the idea that instead of retraining all the layers, what we could do is we freeze everything and we add low-rank matrices, A and B, at the level of some layers, and we're only going to train these matrices to approximate a bigger matrix, a bigger layer of the LLM, and when it's trained, we can just add the frozen weights with lower weights. And experimentally, it works really, really well. Instead of training all of your parameters, you only need to train like 1%, 2% of them. So it's a lot more efficient. And in terms of performance, in terms of accuracy, it's almost as good, which is why it's so popular right now. And so the other question is, when do you actually need to fine tune a model rather than, for example, using an open source model that's already there? When do you decide, okay, now I want to fine tune, I mean, yes, fine tune in a supervised way my own model, or now I'm just going to use an open, an LLM on the, well, open source or closed source? I think there are two main reasons. The first one is that by fine tuning the model, it can make it better in general. So you take an open source model and you make it, like, just as good as ChatGPT, for example. This is one reason, but like, it's not, like some people like that and they compete on the open LLM leaderboard and okay. But like for most people in the industry, for example, the idea is, okay, maybe Lama 2 or Mistral 7b can, is really good in general, but I have a specific task I want to do, or I have a specific domain, for example, if you work in the medical domain, these models have not seen that many samples from this field. They know a lot already, right? But by fine tuning them, you can make them better and better in like medical knowledge in general. So I would say that if you have a domain specific or a task specific problem, it might be interesting to fine tune these models. And on top of that, if you're fine tuning, you can also do retrieval, a retrieval augmented pipeline, but that might be a topic for later. Yeah, actually that was going to be my next question. Like, first of all, what's RAG? Maybe a high level view. We're going on the tangent a bit, but I think it's an interesting topic to discuss. So what's RAG and when would you use RAG rather than fine tuning based on your experience? So the idea behind RAG is that you can add some context in your experience, so you can add some context to your input. So the model already has access to the information that you're talking about. So if I type about a question about a particular disease, it's better if I have the Wikipedia article about this disease in my context, my input. So the model knows a lot about it, just like, okay, it's in the input so I can leverage this information. So it makes the model better in general to answer your questions. And your question was, when do I use RAG? When do I use fine tuning? It's better to do both at the same time. So you would use RAG with a fine tune model. And what you do for RAG pipeline is that you're going to create a database. It can be a vector database. If you use embeddings, it can be a traditional database or otherwise. And you're going to query this database based on the user input in order to retrieve the best context possible and add it to input. There's no rule per se, about when you should use one or the other. It might be more about, do you have the data set to train a supervised model in a supervised way? Because that's not obvious to have such a data set. So RAG might be more useful. And if you can, combining both is where you're going to get the most accuracy possible. So just to summarize, like supervised fine tuning, you basically give some questions, some answers, and you show your, and you train your model. And you show your, and you train your model. And you show your model on this, like a regular machine learning problem. RAG, you have a set of documents and you basically use some algorithms to retrieve the best parts of those documents based on the question that you ask. So if you ask, what pills should I take, given that I have this sickness? If you have a document about the pills that you should take based on the sickness, it's going to retrieve the part of the document related to your question. And so you don't actually need to train your model or anything. It's just some kind of retriever that's going to, as you mentioned, embed your question and retrieve the best part of this document. So those are like the two different approach, two things that you could do. From my experience, I feel like RAG is the simplest way. So if you want to start with something, start with RAG because you don't need to collect a data set. You just need a bunch of documents. You embed them and then you check if it works. If it doesn't work, then I think, as you mentioned, we can try both just fine tuning or fine tuning plus RAG. I think that's a good suggestion. Yeah, anyway, you will want RAG at some point, so you can start with it anyway. At least it's done. OK, so the third step now after having trained the model, I mean, actually, we're still on the second part, the LLM scientists, but the third step is, first step is pre-training, then we do some supervised fine tuning, and after that, there is RLHF and now what you mentioned, DPO. So again, can you mention why this third step is needed? Because after the second step, the LLM should be able to answer questions, right? That's what the second step is about. So why do we need a third step? And maybe we can dive a bit more into the difference between RLHF and DPO. Most of the time, these companies, like OpenAI, they use it to censor the models. So they want to make sure that if you're saying something, like I want to create a bomb, they're going to have it in their preference data set, and they're going to have this instruction, I want to create a bomb, and they're going to have a rejected answer, which would be, OK, like let me tell you all the steps that you need to make a bomb. And the preferred answer will be, no, as an AI assistant, I cannot do that. And when you see, like, as an AI assistant, I cannot do that, or I do not have emotions, you can think about, I'll let Jeff, because this is very, this is probably what they used to instill this behavior into the model. So DPO is a more flexible. Sorry to cut you. Before going into DPO, can you again explain what's RLHF, just in case people aren't too familiar with it? Like, I understand there are those preferences, but how does it work? I think this will help a bit more understanding DPO later. Yeah. It's actually, like, very similar. I think DPO is easier to understand, because there's no reinforcement learning, which is the most difficult part. But reinforcement learning from human feedback is the idea that you have this human feedback, and you use it to improve the model. So it's not just about, like, a fixed training set. Now you can have, like, a feedback loop, where you have a model that is trained to be able to do the same thing. And to imitate the behavior of a human, and it will provide some rewards to a second model. And this is the one that you're actually training to instill this behavior. OK. OK. That makes sense. And so now DPO. Sorry, I cut you. Go ahead with DPO. DPO is really, like, it's a lot more simple, because this time you don't need really rewards or anything. It's really, you can use the same model twice. And the idea is, yeah, you have these preferred answers and rejected answers. And this is what the model is going to learn, right? So for each instruction, you have two options. And the model will be rewarded for outputting answers that look like the preferred ones. And it's going to be punished for outputting answers that look like the rejected ones. OK, cool. So I think now we have, like, a good understanding of all the things. So let's talk about the LLM scientists parts of the course, like from pre-training to RLHF and DPO. The last part that you mentioned, which is something that you've worked on, you build your own LLM by merging LLMs together. So do you want to talk a bit about this part? How does this work? Why is it so powerful and so commonly used today? Yeah, so the idea behind merging models, what's nice about it is that you don't even need a GPU anymore. You just need a CPU, and you just need to merge weights. So it's a lot less computationally expensive. On top of that, it also provides excellent performance, because you can take just two models that were fine-tuned, and the fine-tuning part was really intense. Cost a lot of money, a lot of effort. And by just taking two of these models, you can make a better one. So there's a lot of incentives in doing it. And now the community is really, really, really, really, really, really into into the community. It's really like merging, merges, and merges of merges, and merges of merges. So a little project I've started is this Model Family Tree, where you can input the name of a model and see all the parent models. So it's like a family tree, really. And sometimes you're really surprised, because you see all of these generations of models, like a lot of models have been merged into one. this is really funny and creative and people come up with new techniques and yeah this is interesting for the field the only problem is that sometimes you have contaminated models that have been trained on the test sets so on the benchmark that are using the OpenLM leaderboard and those are not removed so people start using them and now you have merges including these models and merges of merges and merges of merges of merges so now it's really difficult and you can see like all the top models that leave the board they're all contaminated in one way or the other which is a bad thing because now we cannot really rely on the OpenLM leaderboard but the good thing about it is that they seem to still perform better so you know like just merging models over and over again seems to really compress the information and make them better you know like just merging models over and over again seems to really compress the information and make them better so even if they're contaminated I would say do not discard them you can try them and they're probably better than the non contaminated models which is a bit unfair is it kind of the same idea as a random forest basically where you've got lots of trees and you average the prediction so is that kind of what this merge kit and merging LLMs is about you think or is it something different no I think it's a good metaphor I think that it's not really ensembling because done but I think that it's not really ensembling because different elements and you're going to combine the outputs which is also an to use of experts and not so much with merging with merges what you really do is but here it's closer to mixed use of experts and not so much with machine it's not really easy because with merging what you do is that you take all it's not really easy because with merging what you do is that you take all the weights all the parameters and you combine them in a clever way so it's all the parameters and you combine them in a more sophisticated way so it's not just average the system and you have a j goggles and it's spherical linear interpolation so it's a spherical averaging and this tends to work really really well like it's difficult to outperform it and then you have more interesting and clever algorithm that allow you to merge multiple models and and do some things related to task vectors to really only keep the most relevant parameters instead of merging everything and that's really interesting I think it's going to be a big trend in 2024. So one last question on GenAI which is the last part of this pipeline like once you've built a model you often want to deploy it in production so what are the steps here required and what are the common challenges in deploying an LLM in production compared to deploying a traditional model? For example? Yeah there are lots of different tools that can help you to deploy it it really depends if you're a hobbyist or if you do it like for a company if you do it for a company you probably already have a lot of tools maybe you use AWS maybe you use like a Azure or another cloud in this case you're going to you probably want to use the same ones and it tends to be very dependent on your environment so it's difficult to provide a general answer there's there's a guy working at HoudinkFace Phil Schmidt he has an excellent blog if you're interested in SageMaker because he has a lot of content about how to do it with AWS and this is what I would recommend in this case and if you're just a hobbyist maybe you don't need that maybe you can just like quantize the model with LLMA.cpp and gguf to be able to run it locally on your machine and share it with other people who are going to do the same you can use LM studio you can use OLama for example to run them and otherwise you can also create demos using Gradio and you can host it on HoudinkFace you can use Streamlit too and host it somewhere on the web so there are really a lot of different ways to deploy these models it really depends on what's your use case how you want to consume it but in general if you're just a hobbyist I would say local inference is the best way of doing it okay great perfect well thanks a lot let's close this video here and I'll see you in the next one. I'm going to start a new question that I want to ask all my guests which is that this show is called AI Stories so can you share one story about yourself like it could be anything something funny something that you've learned a mistake that you've made whatever but yeah one story about yourself. something that people do not know about me is that I've really started computers and computer science because I was really into game development and this is really what motivated me to learn about computer science and even get into AI because I get the idea my mind was like oh yeah I can reuse it for my games and I spent like years and years doing this indie game development and I can say with some experience that I have learned from my game development I think that this is really the life for me and I also learned a lot from the games because I've been playing for years and I spent years and years doing this indie game development and I can say with some certainty that it's a lot harder than AI. I have really a lot of respect for any indie game developer that can ship a game, like a good game on Steam, because the insanity that is the work required to build a game is like, I don't know, like instead of building one game, you can redo GPT-3 from scratch by yourself. It's really extremely time consuming. And yeah, doing AI is like a hobby. It's really like a pleasure compared to indie game development. So I would say I have all my respect to indie game developers that are listening. And I learned a lot about computer science, about algorithms through this experience. So this is something that I really, really enjoy. But right now, when I see how it's going to be like, I'm going to be like, I'm going to be like, I'm going to be like, it is to just write an article and ship it, fine tune a model and ship it, compared to building a game for years and never ship it. Yeah, I think that I'm more comfortable in AI now. Do you think that this made you a better machine learning engineer? I think so, in a way, yes, because I had to develop a lot of my stuff. So it really shows you a lot of different algorithms and things that I was able to use. But AI in games, is really different from machine learning and projects trying to implement machine learning for games. But basically, I don't know, it doesn't work that well. It's still very niche. That's what I want to say. It's still a very niche application. And yeah, all the time, I keep thinking about how can I use LLMs and other AI techniques, not to just create assets for me, but instead to really change the way that we do game design. The way that we play games, because you have, for example, this demo that was very popular a few months ago, where you have to convince an LLM to give you a secret password. And I think this is a really cool idea, because it really is a game, like you have to convince someone by typing, like a smart text. So this is something that is not really possible without LLMs, right? So this would be a good example. But unfortunately, I'm not very creative, apparently, because I cannot come up with it. But I think it's a really cool idea. And I think it's a really cool idea. But I'm sure that there are a lot of things to explore in this space. And yeah, I think that in a few years, we'll be playing a lot of games that embed AI in their systems. And that can be really, really interesting to see how far we can push the boundaries of game design and really explore new types of games that were completely impossible to play just a few years ago. Another topic that we didn't really discuss, I didn't mention that you wrote a book about the AI, but you wrote about the AI. And I think that you're writing a lot of articles. What do you think of the importance of writing as a machine learning engineer, as someone in tech? Yeah, what do you think about writing? To me, it was really a life hack. Because by writing about these topics, it forces you to learn a lot more about them. Because you're always scared that, oh, no, I'm gonna write something wrong. And people are going to make fun of me on the internet. And it never happened, actually. Actually, yeah, I made mistakes, of course. But people just kindly posted a comment, or they emailed me to tell me, hey, this is wrong, actually. And then I was able to fix it. But yeah, I think that when you write an article, you want it to be perfect. And because of that, you really have to learn a ton and be very sure of what you say. And sometimes the information is not accessible, and you have to delve into the code. This is what I had to do with an article I wrote about. Lama.cpp, for example. I had to really read the C++ code in order to understand what was happening, because no one documented it. Which is quite funny, but also really shows you the extent that it really pushes you to learn this stuff. And by learning, you become better at it. And by communicating about it, people also think that you're good at that. So it's also really, really good for your career. And this is what I've started applying. So yeah, thanks a lot for that. Let's finish the episode with just one advice. If you had one advice for someone to progress in their career, what would it be? Just one advice? Well, I wrote articles. So as someone who's a Kaggleground master would say, like, hey, you should do more competition. I would say, like, write more articles, because to me, it's really important. And I think that's what I'm trying to do. I think that's what I'm trying to do. And I think that's what I'm trying to do. I think that's what I'm trying to do. And I think that's what I'm trying to do. It's really, really useful. But in general, I would say, try to find a project that really motivates you. And just try to build it and ship it. And there's a lot of value in building your own project. And there's a lot of value in shipping it and doing the presentation, the marketing, really, like to sell it to other people. And people online are not that mean. Actually, they're quite friendly, especially in the LLM community. I can say that people are super, super friendly. So really feel free, feel welcome to ship your projects. And the community will probably welcome you and accept them with a lot of kindness. Well, Maxime, thanks a lot. It was a pleasure to have you and learn from you. Have a great evening in London. And yeah, hope to catch up very soon. Thanks, Neil. Thanks for inviting me and have a great day. Thank you.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = result[\"text\"]"
      ],
      "metadata": {
        "id": "VrIZJt0vhIDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q langchain langchain-openai langchain_core langchain-community langchainhub openai ragas tiktoken cohere faiss_cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-RFpDbTYzqY",
        "outputId": "71f8d874-abae-418b-de1d-b5420da833da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.5/807.5 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.9/256.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "print(f\"LangChain Version: {langchain.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kW_Y4mGXY3fE",
        "outputId": "e8d6ddd8-78ae-4547-a7ff-103cc502522d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangChain Version: 0.1.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from getpass import getpass\n",
        "\n",
        "openai.api_key = getpass(\"Please provide your OpenAI Key: \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQaMLWUSY3i9",
        "outputId": "1acd70d6-3b72-4b50-c6f0-9624af52a5f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please provide your OpenAI Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n"
      ],
      "metadata": {
        "id": "hdzzYkAtY3mP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"You are an assitant that reads text generated from a youtube conversation.\n",
        "Please provide detailed list of topics covereed with regard to ONLY on LLMs and steps involved in LLM workflow in bulllet points along with brief description of each main topic to be used for Linkedin post':\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "mJHt74SOis6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "id": "vjTAi9aah6Sp",
        "outputId": "a2c48336-c4b6-4a20-8663-8ddf34ba85d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"You are an assitant that reads text generated from a youtube conversation. \\nPlease provide detailed list of topics covereed with regard to ONLY on LLMs and steps involved in LLM workflow in bulllet points along with brief description of each main topic to be used for Linkedin post':\\n\\nContext:\\n{context}\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "model = ChatOpenAI(model=\"gpt-4\")\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "chain = prompt | model | output_parser"
      ],
      "metadata": {
        "id": "cbcayD0sY3qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"context\": text[0:9999]})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "id": "L_abIolBiW3y",
        "outputId": "c5958c44-6e8d-42fa-a4a6-5b328a899ad5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Main Topics:\\n\\n1. **LLM Workflow**:\\n   - **Pre-training**: The first step in the LLM pipeline, where the model is initially trained on large amounts of data.\\n   - **Supervised fine-tuning**: The second step in the LLM pipeline, where the pre-trained model is further trained on specific tasks with supervision.\\n   - **Reinforcement Learning from Human Feedback**: The third step in the LLM pipeline, where the model learns from human feedback to improve its predictions and decisions.\\n   - **Merging Models**: The fourth step in the LLM pipeline, where different models are merged to improve performance. This process was initially received with skepticism but has proven to be effective and now dominates the Open LLM leaderboard.\\n\\n2. **AI vs Indie Game Development**: The speaker compares the complexities of AI development to indie game development, stating that the former is less time-consuming and more enjoyable.\\n\\n3. **Maxim Labon's Journey in AI**: Maxim Labon, a renowned expert in GenAI and developer of the best performing 7 billion parameter model on the Open LLM leaderboards, shares his journey from studying cybersecurity to falling in love with AI during his PhD. His interest in AI was fueled by the abundance of educational resources and the diverse applications of AI.\\n\\n4. **AI Applications in Cybersecurity**: Maxim discusses his PhD project on intrusion detection systems using machine learning. He also mentions how he applied large language models to network protocols during his time at Airbus.\\n\\n5. **Large Language Models and Network Protocols**: Maxim elaborates on how large language models can be used to understand and classify network protocols. He shares that these models could outperform humans in recognising protocols due to their ability to process large amounts of data.\\n\\n6. **AI in Computer Networks**: Maxim discusses his work at Airbus, where he started with AI applications in computer networks and gradually diversified to other applications. One of his notable projects involved applying large language models to network protocols.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Main Topics:\n",
        "\n",
        "1. **LLM Workflow**:\n",
        "   - **Pre-training**: The first step in the LLM pipeline, where the model is initially trained on large amounts of data.\n",
        "   - **Supervised fine-tuning**: The second step in the LLM pipeline, where the pre-trained model is further trained on specific tasks with supervision.\n",
        "   - **Reinforcement Learning from Human Feedback**: The third step in the LLM pipeline, where the model learns from human feedback to improve its predictions and decisions.\n",
        "   - **Merging Models**: The fourth step in the LLM pipeline, where different models are merged to improve performance. This process was initially received with skepticism but has proven to be effective and now dominates the Open LLM leaderboard.\n",
        "\n",
        "2. **AI vs Indie Game Development**: The speaker compares the complexities of AI development to indie game development, stating that the former is less time-consuming and more enjoyable.\n",
        "\n",
        "3. **Maxim Labon's Journey in AI**: Maxim Labon, a renowned expert in GenAI and developer of the best performing 7 billion parameter model on the Open LLM leaderboards, shares his journey from studying cybersecurity to falling in love with AI during his PhD. His interest in AI was fueled by the abundance of educational resources and the diverse applications of AI.\n",
        "\n",
        "4. **AI Applications in Cybersecurity**: Maxim discusses his PhD project on intrusion detection systems using machine learning. He also mentions how he applied large language models to network protocols during his time at Airbus.\n",
        "\n",
        "5. **Large Language Models and Network Protocols**: Maxim elaborates on how large language models can be used to understand and classify network protocols. He shares that these models could outperform humans in recognising protocols due to their ability to process large amounts of data.\n",
        "\n",
        "6. **AI in Computer Networks**: Maxim discusses his work at Airbus, where he started with AI applications in computer networks and gradually diversified to other applications. One of his notable projects involved applying large language models to network protocols."
      ],
      "metadata": {
        "id": "hQ46EHbtkAHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "1. **LLM Workflow**:\n",
        "   - **Pre-training**: The first step in the LLM pipeline, where the model is initially trained on large amounts of data.\n",
        "   - **Supervised fine-tuning**: The second step in the LLM pipeline, where the pre-trained model is further trained on specific tasks with supervision.\n",
        "   - **Reinforcement Learning from Human Feedback**: The third step in the LLM pipeline, where the model learns from human feedback to improve its predictions and decisions.\n",
        "   - **Merging Models**: The fourth step in the LLM pipeline, where different models are merged to improve performance. This process was initially received with skepticism but has proven to be effective and now dominates the Open LLM leaderboard.\n"
      ],
      "metadata": {
        "id": "yZrEQC20knxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template2 = \"\"\"You are an assitant that reads text generated from a youtube conversation.\n",
        "Please provide list of topics covereed with regard to ONLY steps involved in running LLMs in Production in bulllet points along with brief description of each main topic to be used for Linkedin post':\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "rKXgKFIknLaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_template(template2)\n",
        "chain = prompt | model | output_parser\n",
        "chain.invoke({\"context\": text[0:9999]})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "bMWLRXsto8aP",
        "outputId": "991f14cd-72d9-4af7-df01-1408b39df341"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"1. **Pre-training**: This is the initial stage in running large language models (LLM) in production. It involves training the model on a large corpus of text data, which helps it learn the structure of the language and understand the context.\\n\\n2. **Supervised fine-tuning**: This is the second stage where the pre-trained model is further refined on a specific task with labelled data. This step helps the model to adapt its knowledge from the pre-training phase to a more specific task or domain.\\n\\n3. **Reinforcement learning from human feedback**: This is the third stage where the model learns to improve its predictions based on feedback from human interactions. This step helps the model to continually refine and improve its performance based on real-world interactions and feedback.\\n\\n4. **Merging models**: This is the final stage where multiple models are combined to form a more powerful and effective model. This process can often result in a model that outperforms any of the individual models it was created from. \\n\\n5. **Implementation and continual learning**: After these steps, the model is ready to be deployed in a production environment. However, the learning process continues as it interacts with real-world data, and any insights gained from this can be used to further fine-tune and improve the model. \\n\\nNote: The speaker also briefly mentioned indie game development and its challenges compared to AI development, but it's not directly related to running LLMs in production.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    }
  ]
}